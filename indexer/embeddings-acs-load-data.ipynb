{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=mXH7FUIfoMs\n",
      "[youtube] mXH7FUIfoMs: Downloading webpage\n",
      "[youtube] mXH7FUIfoMs: Downloading ios player API JSON\n",
      "[youtube] mXH7FUIfoMs: Downloading android player API JSON\n",
      "[youtube] mXH7FUIfoMs: Downloading m3u8 information\n",
      "[info] mXH7FUIfoMs: Downloading subtitles: en\n",
      "[info] mXH7FUIfoMs: Downloading 1 format(s): 313+251\n",
      "Deleting existing file data/mXH7FUIfoMs.en.vtt\n",
      "[info] Writing video subtitles to: data/mXH7FUIfoMs.en.vtt\n",
      "[download] Destination: data/mXH7FUIfoMs.en.vtt\n",
      "\u001b[K[download] 100% of  295.53KiB in \u001b[1;37m00:00:00\u001b[0m at \u001b[0;32m1.69MiB/s\u001b[0m\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the video's auto-generated subtitles and save them to a file\n",
    "!mkdir -p bin\n",
    "!mkdir -p data\n",
    "!if [ ! -f bin/youtube-dl ]; then curl -L https://github.com/yt-dlp/yt-dlp/releases/download/2023.06.22/yt-dlp -o bin/youtube-dl; fi\n",
    "!chmod a+rx bin/youtube-dl\n",
    "\n",
    "video_id = \"mXH7FUIfoMs\"\n",
    "!youtube-dl --write-auto-sub --sub-lang en --skip-download -o \"data/{video_id}\" https://www.youtube.com/watch?v={video_id}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b620a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/\n",
      "Requirement already satisfied: webvtt-py in ./lib/python3.8/site-packages (from -r requirements.txt (line 1)) (0.4.6)\n",
      "Requirement already satisfied: azure-identity==1.13.0b4 in ./lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.13.0b4)\n",
      "Requirement already satisfied: azure-search-documents==11.4.0a20230509004 in ./lib/python3.8/site-packages (from -r requirements.txt (line 4)) (11.4.0a20230509004)\n",
      "Requirement already satisfied: azure-ai-formrecognizer==3.2.1 in ./lib/python3.8/site-packages (from -r requirements.txt (line 5)) (3.2.1)\n",
      "Requirement already satisfied: azure-storage-blob==12.14.1 in ./lib/python3.8/site-packages (from -r requirements.txt (line 6)) (12.14.1)\n",
      "Collecting openai[datalib]==0.26.4 (from -r requirements.txt (line 7))\n",
      "  Using cached openai-0.26.4-py3-none-any.whl\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.11.0 in ./lib/python3.8/site-packages (from azure-identity==1.13.0b4->-r requirements.txt (line 2)) (1.27.1)\n",
      "Requirement already satisfied: cryptography>=2.5 in ./lib/python3.8/site-packages (from azure-identity==1.13.0b4->-r requirements.txt (line 2)) (41.0.1)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.20.0 in ./lib/python3.8/site-packages (from azure-identity==1.13.0b4->-r requirements.txt (line 2)) (1.22.0)\n",
      "Requirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in ./lib/python3.8/site-packages (from azure-identity==1.13.0b4->-r requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./lib/python3.8/site-packages (from azure-identity==1.13.0b4->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: azure-common~=1.1 in ./lib/python3.8/site-packages (from azure-search-documents==11.4.0a20230509004->-r requirements.txt (line 4)) (1.1.28)\n",
      "Requirement already satisfied: isodate>=0.6.0 in ./lib/python3.8/site-packages (from azure-search-documents==11.4.0a20230509004->-r requirements.txt (line 4)) (0.6.1)\n",
      "Requirement already satisfied: msrest>=0.6.21 in ./lib/python3.8/site-packages (from azure-ai-formrecognizer==3.2.1->-r requirements.txt (line 5)) (0.7.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in ./lib/python3.8/site-packages (from azure-ai-formrecognizer==3.2.1->-r requirements.txt (line 5)) (4.7.0)\n",
      "Requirement already satisfied: requests>=2.20 in ./lib/python3.8/site-packages (from openai[datalib]==0.26.4->-r requirements.txt (line 7)) (2.31.0)\n",
      "Requirement already satisfied: tqdm in ./lib/python3.8/site-packages (from openai[datalib]==0.26.4->-r requirements.txt (line 7)) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in ./lib/python3.8/site-packages (from openai[datalib]==0.26.4->-r requirements.txt (line 7)) (3.8.4)\n",
      "Requirement already satisfied: numpy in ./lib/python3.8/site-packages (from openai[datalib]==0.26.4->-r requirements.txt (line 7)) (1.24.4)\n",
      "Requirement already satisfied: pandas>=1.2.3 in ./lib/python3.8/site-packages (from openai[datalib]==0.26.4->-r requirements.txt (line 7)) (2.0.3)\n",
      "Requirement already satisfied: pandas-stubs>=1.1.0.11 in ./lib/python3.8/site-packages (from openai[datalib]==0.26.4->-r requirements.txt (line 7)) (2.0.2.230605)\n",
      "Requirement already satisfied: openpyxl>=3.0.7 in ./lib/python3.8/site-packages (from openai[datalib]==0.26.4->-r requirements.txt (line 7)) (3.1.2)\n",
      "Requirement already satisfied: docopt in ./lib/python3.8/site-packages (from webvtt-py->-r requirements.txt (line 1)) (0.6.2)\n",
      "Requirement already satisfied: cffi>=1.12 in ./lib/python3.8/site-packages (from cryptography>=2.5->azure-identity==1.13.0b4->-r requirements.txt (line 2)) (1.15.1)\n",
      "Requirement already satisfied: PyJWT[crypto]<3,>=1.0.0 in ./lib/python3.8/site-packages (from msal<2.0.0,>=1.20.0->azure-identity==1.13.0b4->-r requirements.txt (line 2)) (2.7.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.0 in ./lib/python3.8/site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity==1.13.0b4->-r requirements.txt (line 2)) (2.7.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./lib/python3.8/site-packages (from msrest>=0.6.21->azure-ai-formrecognizer==3.2.1->-r requirements.txt (line 5)) (2023.5.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in ./lib/python3.8/site-packages (from msrest>=0.6.21->azure-ai-formrecognizer==3.2.1->-r requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: et-xmlfile in ./lib/python3.8/site-packages (from openpyxl>=3.0.7->openai[datalib]==0.26.4->-r requirements.txt (line 7)) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./lib/python3.8/site-packages (from pandas>=1.2.3->openai[datalib]==0.26.4->-r requirements.txt (line 7)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./lib/python3.8/site-packages (from pandas>=1.2.3->openai[datalib]==0.26.4->-r requirements.txt (line 7)) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./lib/python3.8/site-packages (from pandas>=1.2.3->openai[datalib]==0.26.4->-r requirements.txt (line 7)) (2023.3)\n",
      "Requirement already satisfied: types-pytz>=2022.1.1 in ./lib/python3.8/site-packages (from pandas-stubs>=1.1.0.11->openai[datalib]==0.26.4->-r requirements.txt (line 7)) (2023.3.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./lib/python3.8/site-packages (from requests>=2.20->openai[datalib]==0.26.4->-r requirements.txt (line 7)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./lib/python3.8/site-packages (from requests>=2.20->openai[datalib]==0.26.4->-r requirements.txt (line 7)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./lib/python3.8/site-packages (from requests>=2.20->openai[datalib]==0.26.4->-r requirements.txt (line 7)) (2.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./lib/python3.8/site-packages (from aiohttp->openai[datalib]==0.26.4->-r requirements.txt (line 7)) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./lib/python3.8/site-packages (from aiohttp->openai[datalib]==0.26.4->-r requirements.txt (line 7)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./lib/python3.8/site-packages (from aiohttp->openai[datalib]==0.26.4->-r requirements.txt (line 7)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./lib/python3.8/site-packages (from aiohttp->openai[datalib]==0.26.4->-r requirements.txt (line 7)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./lib/python3.8/site-packages (from aiohttp->openai[datalib]==0.26.4->-r requirements.txt (line 7)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./lib/python3.8/site-packages (from aiohttp->openai[datalib]==0.26.4->-r requirements.txt (line 7)) (1.3.1)\n",
      "Requirement already satisfied: pycparser in ./lib/python3.8/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity==1.13.0b4->-r requirements.txt (line 2)) (2.21)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./lib/python3.8/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-ai-formrecognizer==3.2.1->-r requirements.txt (line 5)) (3.2.2)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.27.8\n",
      "    Uninstalling openai-0.27.8:\n",
      "      Successfully uninstalled openai-0.27.8\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "semantic-kernel 0.3.1.dev0 requires openai<0.28.0,>=0.27.0, but you have openai 0.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed openai-0.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d310c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:00 hey everyone in this video I want to  talk about Azure container storage  because we always talk about containers  we talk about pods and we think of them  as chickens we have this pod and this  pod Springs into life  it does some specific function then it  disappears so they may be very  short-lived but that's not always the  case sometimes we have containers that  have to have some durable state  maybe it's running a database but it has  to be able to have this storage that  persist beyond the life cycle of the  container and so we absolutely have  these scenarios where hey I do need some  storage  that it can use  that is durable  and we have mechanisms to do that today  so the way this would work right now is  well we have this concept of persistent  volumes\n",
      "01:00:00 and that persistent volume is backed by  some durable storage in the Azure world  if it was just single access it could be  an Azure disk if it was multi-pod access  I can use things like Azure files Azure  NetApp files also there's block with NFS  V3 so there's different types of ways  that I can have some drawable storage  now there are other types of volumes  there are for example the idea of key  values there are secrets there's the  empty directory but I'm really thinking  about just regular hey I need some area  to have some durable storage  now there are built-in storage classes  so we have this idea of storage classes  and as I'm talking about there are ones  that are built in  and I can also create custom ones  but what I then do is that the Pod says  hey I need a certain type A Certain\n",
      "02:00:00 specifications so the Pod links to  persistent volume claim which references  a storage class could be a built-in one  or it could be custom one I create  that will then go  when I create the Pod that uses the PVC  the P persistent volume will spring into  life and hey  it maps that drawable story so the PVC  maps to a persistent volume so this is  one to one binding between them  but a key Point here is  for all of this to work  what's really happening around all of  this obviously is this pod  well that pod  is running  on a node we have the container host so  there's running a node here  and when we talk about this mapping to  the durable storage what actually  happens is this persistent volume is  surfaced by a mount on the Node\n",
      "03:00:00 so if we think about what has to happen  here is when that persistent volume  actually Springs into life that's being  backed by some durable storage  well there are resource providers in  Azure that have to get called and do a  job if it's using managed disk has to go  and create the manage disk  maybe I'm going creating a new file  share whatever that might be I'm calling  resource providers to do certain  operations once that is done well all of  this access has to go via the node and  so then the node has to do work the node  has to mount the managed disk the node  has to connect to the SMB share or the  NFS share  well that all takes time and that only  happens when I create the pod  that has the PVC that then maps to the  persistent volume that makes it spring  into life unless I'm doing the custom  and I'm pre-creating in advance so if  you think about that every time I have a\n",
      "04:00:00 pod that requires that durable storage  there's a certain amount of latency that  gets introduced because the resource  providers have to do work the node has  to do work and it also limits what I can  do because it is the node that is  attaching to this durable storage  so that limits me in terms of certain  types of scale I can do it limits me in  terms of the performance the cost hey  every pod that wants a persistent volume  that's using a desk where it needs its  own disk  because we're really taking  infrastructure as a service type  resources  and we're using orchestration to get  them to work with AKs that does add some  limitations like for example a disk is  typically a one-to-one that rapid  scaling can be tough and I'll get a  different experience depending on what  is that back-end durable storage because  we have all of those different options  and also realize I am bound to whatever  this capability is if it was a managed\n",
      "05:00:00 disk maybe I only need a small amount of  capacity if I need huge iops and huge  throughput I may be limited by that I'd  have to create a much bigger disk and an  un wasting resource so the challenges  with this direct approach that we have  here  and so as you'd expect then  the solution is enter Azure container  storage now this is derived from the  open source solution open EBC so it  provides a managed volume orchestration  solution now as we'll see as we go  through this it is deployed into the  node pool so it's going to create some  pods on the Node pool but there's a  number of different components that make  up the Azure container storage solution  the biggest one we're going to see is  the idea of a storage pool  and this is an abstraction and that  powers a lot of what we do because if we  think about what we're still going to  have most of the constructs remain the  same hey we still have the Pod the Pod\n",
      "06:00:00 is still going to reference a persistent  volume claim it wants  we're still going to have a storage  class  still the durable storage  that can't go away I still need  somewhere to actually store it  but what's going to change here is the  storage class now talks to a storage  pull  now that storage Paul has different  characteristics like hey what's its size  what's its type because there are  different types of backing storage it  can use  and it's the storage pool but also the  oval pods and the solution of azure  container storage that will actually go  and talk to the storage it's not some  mount on the Node anymore  so we add this abstraction with the  storage pool obviously there still is  the idea of a persistent volume is going\n",
      "07:00:00 to get created using up the capacity and  the capability that there's the storage  pool but we're adding this in the middle  and we're taking out that Reliance on  the Node having to be the one to go and  connect to the storage so that's really  the huge shift so the Azure container  storage introduces this content of the  storage pool now it also has a data  services layer so maybe it doesn't do it  today this is early days of time of  recording this video but I could think  about ideas of replication of other  encryption of other functionality  and then of course there's different  protocol layers for how the solution  will go and talk to the underlying  durable storage because if it's not the  node that's doing a mount and attach to  the managed disk it has to go and talk  to it so we hear things about nvme over  fabric  we talk about iSCSI so we have all these  different things  and so the storage pool really is\n",
      "08:00:00 everything it is this abstraction so  it's the abstraction of the Pod from  their underlying storage  and what's really happening here is when  we think about this storage pool it's at  the point we create the storage pool  which is when those interactions with  arm and the resource provider have to  happen so in some ways I think of it a  lot as reordering the sequence of  operations  with the traditional model the resource  providers the disk creation the  attachment happens at the time the Pod  Springs to life so the Pod takes that  hit on all those things having to happen  when I create the pod that's using the  PVC at that point that persistent volume  that was waiting says okay now I'm doing  stuff I'll go and create whatever  here  we're changing that  we as the administrators of the solution  we're creating the storage pool up front\n",
      "09:00:00 so at that point when we create the  stories before it's going to take some  time that's when the resource providers  will get calls if we're using disk for  example or elastic sand it's going to go  and create the volumes and it can do  that dynamically  it's going to have the ACs components go  and connect and be ready to use  so then when the pods brings to life  that uses a PVC that talks to the  storage class that uses the pool  there's no resource provider operations  there's no connecting to storage  required it says okay you I'm going to  give you a bit of this space over here  and you're done so it's going to be far  far more performant and because I'm  decoupling what it wants from the  underlying drawable hey I could have  I need really high performance but a  small amount of capacity because it's  going to pull all of these resources  so we can now divide it up however it  wants to so that's really I think of the  key point if I think a little bit I\n",
      "10:00:00 think it's like virtualization and the  shift of private cloud  with virtualization we had all these  silos of things it was hard to manage it  took time to create the VM whereas we  went to private Cloud we pulled all the  resources we reordered the operations we  would create these clouds these groups  of resource we'd set up the ability for  people to be able to use a certain  amount so it's streamlined all of the  interactions with it and I think of this  in a it's not the same but I think of it  as a similar idea that we're adding this  abstraction of the storage pool we added  the abstraction of a private cloud  but we're reordering what we do so now  when I want to create that PVC  everything is ready and waiting it's  going to give me a single way to look at  all my storage through the solution I'm  not going to have all these scattered  pieces of storage that are these islands  hey I have this nice pool that all of  these things can share  so I think it's really going to give me  a lot of benefit in my management it's\n",
      "11:00:00 going to reduce any latency when I'm  creating these things and I'm going to  be able to do huge amounts more  connections because maybe I'm not now  limited by nodes and whatever that has  and if I'm adding this abstraction in  the middle or I can add other services  like data protection like recovery like  different types of observability and  monitoring because of this is bringing  everything together  okay so that's the idea  how is it actually working  what what am I going to do as part of  this solution  so we still obviously have the idea of a  node pool so absolutely  I have my Azure kubernetes Services  instance  within an Azure kubernetes these  Services instance a cluster I have a  node pool  so we're operating at node pool levels  so when I think about my node pool now a\n",
      "12:00:00 key Point here is my node pool  needs to have a minimum  of three nodes  three nodes and I'm just going to draw  three obviously I can have more than  three but I must have three  so I've got my three nodes  in this specific node pool  now what happens here is it is Linux  only today so it's Linux only node pools  and the first thing we actually do is we  add a label we add this AC store label  to the node pool  this tells it hey you're gonna host the  various pods and be able to use the  Azure container storage service I can  have multiple node pools in the same  cluster  all with this label so they'd all get  this ACS available and user usable by it  then what I do is on this cluster\n",
      "13:00:00 I'm gonna enable  Azure container storage now I'm not  going to go into the detail of the how  there's documentation that does that but  at time of recording it's in preview and  the way it's on boarding today I don't  think is going to be the same longer  term today it uses Arc it says a few  things we have to do to make that work  just certain permissions it needs  I I doubt it's going to continue that  way so I don't want to go into the  specifics of exactly the install process  because I think it will likely change  but I'm going to enable the Azure  container storage solution and what  that's going to do  is create a bunch of pods  these are pod specific  to Azure container storage solution  which is why we need three minimum now  it does have some overhead on the box I  think it recommends at least\n",
      "14:00:00 One Core will get used by the ACs pod so  it wants a four virtual CPU minimum size  node so there's documentation that says  hey because these are doing work it's  going to use roughly one cause worth of  CPU so we want to make sure we have that  and we can see these so if we jump over  super super quickly let's have a quick  look over here  and what I want to do initially is just  show you the basic pods now I've already  connected to the cluster  and if we just look at the nodes super  quickly  you can see I've got a three node  cluster here now I just want to look up  one of the nodes we look at the first  node and the only reason we're doing  this is I want to just show you  this is the key label so the one thing I  have to do up front is I go and add this  label to the node pool that's what's  going to now make it usable  and buy this\n",
      "15:00:00 and you can see the command I've got  over here and how I'd actually do that  but if I look at all of the pods we'll  see the regular ones that use by the  system  but then we'll see this whole bunch of  pods  that are in this a c s t o r AC store  namespace  and these are what are powering we can  just look at those on their own if we  wanted to  this is what powers it  so I can see all these different things  around metrics and CSI nodes  some of them there's multiple instances  of we can see here  but you can see it does deploy a whole  bunch of PODS to my environment  to enable this to actually run  so we've enabled ACS and it goes and  creates this whole bunch of PODS okay  fantastic now what  so now the next thing we have to do\n",
      "16:00:00 is we create a storage pool  so if we have ACS installed at that  point it's really not done a whole bunch  it's deployed  the Pod so it's ready to go but I need  the storage pool  so if that was my in a way so Step One  is adding this label step two would be  hey I go and enable Azure container  storage solution step three  would be I add  a storage  pull  now it's a day and again I want to  de-emphasize this a little bit so I  think this will change it is the preview  the overall how it works won't change  but the details will probably change  today it supports three types of storage  ephemeral  or and this all word is important or  premium  SSD V1 I manage disk or elastic sand\n",
      "17:00:00 which is also in preview solution  offering iSCSI storage to these so the  key Point here is a particular Azure  container storage instance  supports one type of storage pool now I  could have multiple storage pools of the  same type so within one ACS I'd have  multiple different storage pools of  managed disk I can have multiple  different storage pools of elastic sand  but I couldn't have a storage pool of  elastic sand and one of managed disk  it's  one of these types for a particular  Azure container storage solution  so that's the key point  now it's at the point I create the  storage pool is when it calls the  resource provider so creating the  storage pool will take time that's when  it's actually going to do all of the  work  now how is that and actually maybe we  should go and look at that quick so if  we jump back over again so let's jump\n",
      "18:00:00 back over  if I was to go and look I have created  a storage pool  now firstly destroyed classes  because I have created a storage pool  now what you'll always see is this AC  store Azure disk internal it's always  called Azure disk this is as soon as I  install ACS this is nothing to do with  the storage pool that is just there  what I see special  is I've added a type of azure disk so  now I also see  AC store Dash Azure disk  so that's where I now have that idea of  the storage class  so it's created that storage class  because I've chosen disk as the type of  storage box I went and had and created  that storage pool so if I was to look at\n",
      "19:00:00 my storage pool  let's go over here  we can see I have a one Tabby byte  storage pool  and I've called it Azure disk so I have  a storage pool in my environment now it  was the point in which I created the  storage pool it would have actually gone  and created in this case the managed  disk so if I was to go and look at the  portal  and we look at our disk cluster  I'm gonna leave that up over there  and as we know what really happens  behind the scenes if we look at the  properties  it has its infrastructure Resource Group  it would be in here there we can see it  it actually created the disk  so this is where it actually created the\n",
      "20:00:00 management so at the point you create  the storage pool is when it goes and  creates the resources  and it doesn't try and attach it to all  the nodes now you will see it attached  to one of the nodes  and the reason for that and I'll talk  about this notice it says managed by and  it's the second note because it's  underscore one the first node is zero  but it doesn't mean the pods have to be  on that when I think of different types  of access to storage  remember we have the concept of the  Block Level i o then we have metadata  and in many cases file systems don't  like the idea of multiple nose trying to  do metadata changes at the same time so  what's happening there is one of the  nodes is responsible for the metadata  but all of the nodes can do the direct i  o to it so it's just an important thing  to understand  now I mentioned this idea of the  different types of storage balls I  wanted to talk about that  but if we look if I want to do a femoral\n",
      "21:00:00 then I have to have a certain type of  virtual machine because ephemeral  is going to use the local nvme storage  so when we think about those we're  really talking about the L Series  because the L Series has the nvme  storage  so I'm going to be wanting to use the  nice L Series virtual machines because  they have those nvme disks in them and  that's what's required to do the  ephemeral storage  so we get this hey nice nvme storage in  those so if I'm not using ephemeral  then I can use  pretty much any type of VM SKU I want  but if I want to use a femoral disk then  I must use the storage optimize that  have those nvme so the picture would  look like this so let's break these down  I'm going to separate into the two types  ephemeral or the where the storage is\n",
      "22:00:00 separate  so obviously I'm still going to have the  concept of the three nodes I'm going to  draw these super quickly  if I'm in the ephemeral  in which case these are kind of the L  Series  and the L Series means I have this  lovely  nvme  local storage  on the notes  so when I create my storage pool  remember we have the ACs pods  on all the nodes remember there's lots  of them  the ACs pods local to each of the nodes  talks to its local storage  but then they all communicate with each  other\n",
      "23:00:00 now today I don't think it's doing any  replication but it is fundamentally  taking all of this and pulling it  so I get this much bigger looking  and or stripe disc  and so if I have a pot over here and I  happen to have some of data over here  it's still going to work it all of the  communication as we'll see is going via  the ACs pods it is not the Pod talking  directly to the storage  so today it's not replicating the  content between them but hey that could  change in the future that's just today  it doesn't do that that's absolutely  something that in a day two they could  go and add that functionality  so that's how that works if I'm using  the ephemeral so that would be good for  a scenario where hey I have pods they  want really low late and see really high\n",
      "24:00:00 performance and maybe I have multiple  pods and they're doing their own  replication  so the data is still drawable Beyond a  single node failing because the pods  themselves are doing replication so then  hey yeah I want that really high  performance I'm not so worried about the  durability of any particular node  because the application workload is  replicating for me  what about the scenario where now we're  talking about using the disk  or we're talking about using the elastic  sand  we have the three nodes  so we're always going to have our three  nodes  foreign  we've got the nice ACS pods there's  always going to be core to everything\n",
      "25:00:00 but now it changes depending on which  one we're doing so firstly we'll think  about a scenario where  actually we use manage disk  so we're going to create a storage pool  of type manage disk  so when I create the storage pool I  specify that the types of a desk and the  size and what's going to happen here is  ACS is going to say well how many disks  do I need to meet that so it may go and  create a couple of managed disks  to meet that requirement  and what happens now  is  those pods that are responsible for the  data Communications  they are going to connect should stick\n",
      "26:00:00 to the same color they're going to  connect  directly  to  the storage and they're going to connect  using  nvme  over fabric so they're going over the  network this disk is not being attached  to every node now we saw it attached to  one of them for the metadata that's  really the coordinator for that but the  actual i o is not it's nvme over fabric  so it's using the fabric to go and talk  to those so all of them have that same  connection to it and we saw that in the  portal but pods can run on any node to  be able to use that now if I was using  elastic sand so let's use a different  color for elastic sand now I do want to  stress it's always or it's not like I  could have both of these but if I chose  instead to use my elastic  has its own benefits of dynamic growth\n",
      "27:00:00 and I have those great iSCSI volumes  well now the connectivity  using nice guzzy  and that's the key point but again and I  this is so so important it's all within  one ACS deployment I can't have both of  these I could have multiple managed  disks pools I have multiple elastic  samples but I can't have hey I'll have  one of each and I'll have an ephemeral  it's always one type within a certain  ACS and certain cluster  okay  now I have a pod  now I actually have a workload  so I've got a pod one  and pod one says hey I'm gonna use that  storage class that maps to the storage  pool and so it's still going to have  this persistent volume claim that maps\n",
      "28:00:00 to the class in a particular type  well at this point so let's say this is  PVC one for example  what actually is happening here  is that when I create the Pod well  there's still this it says it's PVC one  again for a second  that PVC one  I'm gonna map to a persistent volume  and let's say I created the requirement  here was a hundred Megs but this is  terabytes in size well at persistent  volume gets allocated  a certain amount of space on that  pre-allocated stuff that's all it has to  do  and remember I could have pods on all of  these different ones  0.2 pod three remember that's a  different persistent volume claims and  now they can move easily between nodes  with no delay because I'm not having to  try and take some storage and move it  around now the exact models they're\n",
      "29:00:00 going to support in terms of multi-ri  and I think that's again evolving we're  in early days what they have today is  not necessarily what they're gonna have  but certainly they have the capability  to take any type and expose all of them  as all of the multi-right type scenarios  they won't be bound by the restrictions  that we had when the node had to attach  to the storage but his early days what  they have now I don't think is  indicative of what will be there  so now it actually will go and  create this the communication between  the Pod and the storage  is via the ACs it is not talking itself  directly to the storage it's going  through this service which is why this  service could add those other features  and functionalities  so we are adding this abstraction but  it's that abstraction that now gives me  the ability that this could ask for a  really high throughput\n",
      "30:00:00 requirement  and I don't have to go and create some  huge disk because the pool is Big the  pool has a large amount of capacity and  iops and throughput  I can now divide up those different  dimensions differently  I could have someone that needs really  high performance a small amount capacity  someone else needs a lot of capacity but  low performance great they're balancing  each other off  and because I have the Azure container  storage solution I can see how I'm using  the resources I have that observability  it's going to let me tweak and tune and  change those things  however I need to so this is really the  key Point behind all of this and it's  probably easier if we were to just go  and see this  so we saw this idea that okay yeah I had  this disc remember it was on vmss one  just hopefully I can kind of show you  that doesn't actually matter  so if we go to our demo environment\n",
      "31:00:00 let's clear this out  now remember we had the storage pool  it was just that one terabyte I can  describe it I can see the detail around  so it's ready storage pool is ready to  go  if I look at my persistent volumes I  have one right now and you'll notice the  name of the persistent volume  and it's that one Tabby byte it's the  same size as the manage disk so I just  have one persistent volume because  that's how the ACs pods think about  doing some of the mapping remember we  had that metadata we had those various  requirements so this persistent volume  is what's that binding between ACS and  that managed disk so that's what we have  right there  so let's actually go and create  something so if we look at my persistent  volume claim the key part here is my  persistent volume claim where it has a\n",
      "32:00:00 name which they do so it's Azure disk  PVC I'm just using a read write once for  the access mode  this is the important bit  the AC store Azure disk storage class  now this would be whatever I use as part  of when I create my storage pool  but mine is just an AC store Dash Azure  disk so that is my storage pool name and  it's 100 gigabytes so that's the  important part there  and again just in case we forget so that  was my storage pool  so in the namespace and my storage pool  name was Azure disk so that's the  storage class in that file  that I am specifying  so it went and created when we talked  about based on the name of the storage  pool I created it created  a new  storage class so again we look at our\n",
      "33:00:00 storage classes  because I created a storage pool called  AC store  Azure disk it creates  AC store Azure disk and you can see it's  using  containerstorage.csi.azure.com so we  just added AC store Dash to the name of  the storage pool  I created which was Azure disk  right there  so that's the relationship between the  PBC so storage pool was Azure disk it  created a  storage controller  storage class site  called AC store Dash disk and I'm just  referencing that as the storage class  so let's go ahead and create that  and this is all from the Microsoft  examples you could follow this through  yourself  so we will now go\n",
      "34:00:00 and just apply it this will create that  persistent volume claim so we see it  created it super super fast  if we look at them  now this is pending so there's no pod  using it yet  if we describe it  we're going to see it's just waiting so  it's sitting there waiting for the first  consumer to be created before I bother  actually binding so that's a key point  there  so also at this point we wouldn't have  any new persistent volume there's  nothing actually bound to it yet I don't  need to do any actual work  so now let's create the pod  so if we remember the PVC the name was  Azure disk PVC in the pod  what we're doing is well we want to make  sure we want to note that he's got the  AC store label  but we're going to use the claim  Azure disk PVC that we created in the\n",
      "35:00:00 previous step that's the important part  this is going to use  is then going to mount it  over here  so now if we go ahead and apply that  so it's done  if we look at the pod  in detail so the thing I'm hoping to see  let's have a look  yes okay so notice it's running  on node 2 which is the third node  so it's running remember on a node that  is not the same as the node that had the  disk attached that proves it's not bound  to that that's purely that metadata  stuff so we don't actually have any  limitations by that so we have the pod  now if we look at the persistent volume  claim  we can either see it sprung to life  and the important thing that we care  about here that shows that abstraction\n",
      "36:00:00 is this  it's using the container storage CSI so  it's not using some standard just oh  it's the managed disk it's going via the  Azure container storage solution that  really is the key to it so it didn't  create any new managed disks there's  nothing new about it we will see a new  persistent volume  that is for the capacity that we wanted  so we see it created the new PVC  for that 100 gigabyte but it's just  using a portion of the disk  from before if we go and look again  just close that down  and we can refresh  there's no new disk  it's just carved out a portion of the  disk it already created and that's why  it's so fast and it's so flexible now I  can move these around super super easily\n",
      "37:00:00 and again I I specify what the re-plane  policy would be  but I've completely now separated  all of those elements  and that separation is so powerful when  I think about  the scalability now hey I can share this  pool between lots of different pods I  can have pods move around and not have  any delays of trying to have the nodes  connect I have one view across all of  this now I can divide up this pool  however I want I'm not restricted to  what can one disc do  it's bringing it more in line with the  idea  of  what would be a true container storage  solution so I'm moving away from is it  the point the pods created is bound to  this particular resource and what its  capabilities are and then what the  limitations of the node might be in the\n",
      "38:00:00 time those things takes to I'm  reordering the operations I'm adding  this abstraction so yes from the  administrator I'll see that little delay  when I'm creating the storage pause it  does all the stuff behind the scenes  but now that experience when I go and  have a pod that uses a PVC  it's really fast it's really flexible I  have a huge amount more scalability I  can divide up the different  characteristics of that back-end storage  however I want  and that's really the key Point around  this I mean that's what this brings it's  a game changer when I think about hey I  want that performance I want scalability  I want the flexibility I want the single  viewer not have bits of resource all  over the place that I'm trying to manage  it brings it all into this one service  now again  a timer recording this is in preview I  suspect some of the interactions the  type of access modes are going to change  I think the day two services that can\n",
      "39:00:00 modify other benefits this can bring  will just grow  but that's where it is right now  now the question always comes up well  how much does it cost I don't know yet  there will be a cost so it does talk  about it see what details we have but  again you'd want to check the pricing  Pages Linked In the description below  but it is talking about there will be  pricing  Beyond a certain amount  because obviously there's a lot of  investment work and research that goes  into creating this solution  and so it talks about hey there'd be a  free tier  if it's under five Tabby bytes  and then there'll be other orchestration  fees and for other amounts so there'll  be some pricing we just don't know what  it is right now  but that's it  um it was a lot of fun playing around  with this the documentation has the nice  steps everything I did  I took those demos straight from the  documentation so you could go and do  exactly the same thing and see those\n",
      "40:00:00 different things get created see the  resources for yourself it it's really  not that bad to go and play around with  this  but that's the solution  as always I hope this was useful until  next video take care  foreign\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "import webvtt\n",
    "\n",
    "\n",
    "video_id = \"mXH7FUIfoMs\"\n",
    "file = 'data/' + video_id+'.en.vtt'\n",
    "\n",
    "captions = webvtt.read(file)\n",
    "\n",
    "\n",
    "result = defaultdict(str)\n",
    "\n",
    "last_words = []\n",
    "\n",
    "for caption in captions:\n",
    "    start_time = caption.start\n",
    "    hh, mm, ss_ms = start_time.split(':')\n",
    "    ss, ms = ss_ms.split('.')\n",
    "\n",
    "    key = int(hh) * 60 + int(mm)\n",
    "\n",
    "    current_words = caption.text.strip().split()\n",
    "\n",
    "    overlap_index = 0\n",
    "    for i in range(1, min(len(last_words), len(current_words)) + 1):\n",
    "        if last_words[-i:] == current_words[:i]:\n",
    "            overlap_index = i\n",
    "\n",
    "    result[key] += \" \" + \" \".join(current_words[overlap_index:]).strip()\n",
    "\n",
    "    last_words = current_words\n",
    "\n",
    "transscript = \"\"\n",
    "\n",
    "for key in sorted(result.keys()):\n",
    "    minute = f\"{key:02d}:00:00\"\n",
    "    data = f\"{minute} {result[key].strip()}\" \n",
    "    transscript += data + \"\\n\"\n",
    "\n",
    "\n",
    "print (transscript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db6e818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search index already exists\n"
     ]
    }
   ],
   "source": [
    "from azure.identity import AzureDeveloperCliCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import *\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "import openai\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# READ ACS_KEY from ENV\n",
    "acs_key = os.environ.get('ACS_KEY')\n",
    "acs_instance = os.environ.get('ACS_INSTANCE')\n",
    "openai_key = os.environ.get('AZURE_OPENAI_KEY')\n",
    "openai_deployment = os.environ.get('AZURE_OPENAI_EMBEDDING_DEPLOYMENT')\n",
    "openai_instance = os.environ.get('AZURE_OPENAI_INSTANCE')\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = openai_key \n",
    "openai.api_base = f\"https://{openai_instance}.openai.azure.com\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "\n",
    "# search_cred should be TokenCredential\n",
    "auth_credentials = AzureKeyCredential(acs_key)\n",
    "\n",
    "def initialize_search_index():\n",
    "    search_client = SearchIndexClient(endpoint=f\"https://{acs_instance}.search.windows.net/\",\n",
    "                                      credential=auth_credentials)\n",
    "    if \"embeddings\" not in search_client.list_index_names():\n",
    "        index_structure = SearchIndex(\n",
    "            name=\"embeddings\",\n",
    "            fields=[\n",
    "                SimpleField(name=\"vector_id\", type=\"Edm.String\", key=True),\n",
    "                SearchableField(name=\"payload\", type=\"Edm.String\", analyzer_name=\"en.microsoft\"),\n",
    "                SearchableField(name=\"additional_metadata\", type=\"Edm.String\", analyzer_name=\"en.microsoft\"),\n",
    "                SearchField(name=\"vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), \n",
    "                            hidden=False, searchable=True, filterable=False, sortable=False, facetable=False,\n",
    "                            dimensions=1536, vector_search_configuration=\"default\"), \n",
    "                SimpleField(name=\"timestamp\", type=\"Edm.DateTimeOffset\", filterable=True, facetable=True),\n",
    "            ],\n",
    "            semantic_settings=SemanticSettings(\n",
    "                configurations=[SemanticConfiguration(\n",
    "                    name='standard',\n",
    "                    prioritized_fields=PrioritizedFields(\n",
    "                        title_field=None, prioritized_content_fields=[SemanticField(field_name='payload')]))]),\n",
    "                vector_search=VectorSearch(\n",
    "                    algorithm_configurations=[\n",
    "                        VectorSearchAlgorithmConfiguration(\n",
    "                            name=\"default\",\n",
    "                            kind=\"hnsw\",\n",
    "                            hnsw_parameters=HnswParameters(metric=\"cosine\") \n",
    "                        )\n",
    "                    ]\n",
    "                )        \n",
    "            )\n",
    "        print(f\"Initializing search index\")\n",
    "        search_client.create_index(index_structure)\n",
    "    else:\n",
    "        print(f\"Search index already exists\")\n",
    "\n",
    "\n",
    "initialize_search_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12c1d9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIndexed 41, 41 succeeded\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def create_embeddings(transscript):\n",
    "    for line in transscript.splitlines():\n",
    "        timecode = line[:8]\n",
    "        text = line[9:]\n",
    "        content = f\"YouTube-ID: {video_id} \\nTimecode: {timecode} \\nText: {text}\"\n",
    "        videoid = video_id\n",
    "        yield {\n",
    "            \"vector_id\": re.sub(\"[^0-9a-zA-Z_-]\",\"_\",f\"{video_id}-{timecode}\"),\n",
    "            \"payload\": content,\n",
    "            \"vector\": openai.Embedding.create(engine=openai_deployment, input=text)[\"data\"][0][\"embedding\"],\n",
    "        }\n",
    "\n",
    "def index(embedding_data, batch_size=1000):\n",
    "    search_client = SearchClient(endpoint=f\"https://{acs_instance}.search.windows.net/\",\n",
    "                                    index_name=\"embeddings\",\n",
    "                                    credential=auth_credentials)\n",
    "\n",
    "    current_index = 0\n",
    "    current_batch = []\n",
    "    for embedding in embedding_data:\n",
    "        current_batch.append(embedding)\n",
    "        current_index += 1\n",
    "        if current_index % batch_size == 0:\n",
    "            report_status(search_client, current_batch)\n",
    "            current_batch = []\n",
    "\n",
    "    if len(current_batch) > 0:\n",
    "        report_status(search_client, current_batch)\n",
    "\n",
    "def report_status(search_client, batch):\n",
    "    results = search_client.upload_documents(documents=batch)\n",
    "    successful_uploads = sum(1 for result in results if result.succeeded)\n",
    "    print(f\"\\tIndexed {len(results)}, {successful_uploads} succeeded\")\n",
    "\n",
    "embeddings = create_embeddings(transscript)\n",
    "index(embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
